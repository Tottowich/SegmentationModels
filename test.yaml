# This is a model configuration file for the attention unet model.
in_channels: 3 # Number of input channels.
num_classes: 151 # Number of classes in the dataset 
steps: 4 # Depth of the U-Net
decoder: # Decoder configuration
  activation: # Activation function per decoder block
  - relu
  - relu
  - relu
  - relu
  attention: # Cross attention per decoder block
  - true
  - true
  - false
  - false
  attn_drop: # Dropout for cross attention per decoder block
  - 0.0
  - 0.0
  - 0.0
  - 0.0
  context_channels: # Context channels per decoder block, from large to small.
  - 256
  - 128
  - 64
  - 3
  conv1x1: # Number of 1x1 convolutions per decoder block
  - 1
  - 1
  - 1
  - 1
  conv3x3: # Number of 3x3 convolutions per decoder block
  - 1
  - 1
  - 1
  - 1
  embed_dim: # Embedding dimension for cross attention per decoder block
  - 768
  - 768
  - 768
  - 768
  in_channels: # Number of input channels per decoder block
  - 512
  - 256
  - 128
  - 64
  num_heads: # Number of heads for cross attention per decoder block
  - 8
  - 8
  - 8
  - 8
  out_channels: # Number of output channels per decoder block
  - 256
  - 128
  - 64
  - 32
  patch_size: # Patch size for cross attention per decoder block
  - 8
  - 8
  - 16
  - 16
  
encoder:
  acts: # Activation function per encoder block
  - true
  - true
  - true
  - true
  attention: # Self attention per encoder block
  - false
  - false
  - true
  - false
  attn_drop: # Dropout for self attention per encoder block
  - 0.0
  - 0.0
  - 0.0
  - 0.0
  depth: # Depth of each encoder block
  - 3
  - 2
  - 1
  - 1
  dropout: # Dropout for each encoder block
  - 0.0
  - 0.0
  - 0.0
  - 0.0
  embed_dim: # Embedding dimension for self attention per encoder block
  - 768
  - 768
  - 768
  - 768
  img_size: # Image size for self attention.
  - 256
  - 256
  - 256
  - 256
  in_channels: 3
  kernel_size: # Kernel size for each encoder block
  - 3
  - 3
  - 3
  - 3
  norms: # Normalization boolean per encoder block
  - true
  - true
  - true
  - true
  num_heads: # Number of heads for self attention per encoder block
  - 8
  - 8
  - 8
  - 8
  out_channels: # Output channels per encoder block
  - 64
  - 128
  - 256
  - 512
  padding: # Padding for each encoder block
  - 1
  - 1
  - 1
  - 1
  patch_size: # Patch size for self attention per encoder block
  - 16
  - 16
  - 8
  - 8
  pool: # Pooling boolean per encoder block, maxpooling is used.
  - true
  - true
  - true
  - true
  stride:
  - 1
  - 1
  - 1
  - 1

